# ğŸ­ Big Data ETL & Warehouse Modeling Pipeline

This project simulates a complete **enterprise-scale data pipeline** that ingests synthetic transactional logs, processes them using **PySpark**, models a **star schema warehouse** with **SCD dimension logic**, and performs **real-time ML deployment** using **Vertex AI**. All components are developed in **Google Colab** and leverage cloud-scale tools such as **MongoDB Atlas**, **Google Cloud Storage**, and **Vertex AI Pipelines**.

---

## ğŸ” Project Highlights

### 1. ğŸ”„ Synthetic Log Simulation
- Log types: `sales_logs`, `user_activity_logs`, `inventory_events`
- Generated using the `Faker` library
- Structured in JSON-like format for downstream NoSQL ingestion

### 2. â˜ï¸ MongoDB Atlas Ingestion
- Logs are inserted into cloud-hosted **MongoDB Atlas**
- Collections are auto-created on insert
- Enables distributed NoSQL-backed ETL workflows

### 3. âš™ï¸ PySpark-Based ETL
- Logs extracted via **MongoDB Spark Connector**
- Cleaned, transformed, and stored as **partitioned Parquet files**
- Partitioning strategies:
  - Sales â†’ by `region`
  - Activity â†’ by `device`
  - Inventory â†’ by `warehouse_id`

### 4. ğŸ§± Data Warehouse Modeling
- **Star Schema** architecture
- Tables:
  - `fact_sales`: transactional metrics
  - `dim_customer`, `dim_product`, `dim_region`, `dim_time`: surrogate keys
- **SCD Type 1 & Type 2** logic implemented on `dim_product`

### 5. ğŸ“Š SQL Analytics Layer
- Queries run via **Spark SQL temp views**
- KPIs:
  - Monthly revenue trends
  - Top products by revenue
  - Product-category performance

### 6. ğŸ¤– Vertex AI ML Pipeline
- Trains a **regression recommender model**
- Saves and deploys using:
  - `joblib` for serialization
  - Google Cloud Storage for artifact storage
  - Vertex AI for model hosting
- Includes full CI/CD automation with:
  - GitHub Actions
  - Cloud Build
  - Conditional deployment via **Kubeflow Pipelines**

---

## ğŸ“ Folder Structure

```

big\_data\_etl\_project/
â”œâ”€â”€ 1\_log\_simulation/
â”‚   â””â”€â”€ simulate\_logs.py
â”œâ”€â”€ 2\_mongo\_ingestion/
â”‚   â””â”€â”€ upload\_to\_mongo.py
â”œâ”€â”€ 3\_pyspark\_etl/
â”‚   â”œâ”€â”€ sales\_etl.py
â”‚   â”œâ”€â”€ user\_activity\_etl.py
â”‚   â””â”€â”€ inventory\_etl.py
â”œâ”€â”€ 4\_data\_warehouse/
â”‚   â””â”€â”€ warehouse/
â”‚       â”œâ”€â”€ sales\_logs/
â”‚       â”œâ”€â”€ user\_activity\_logs/
â”‚       â”œâ”€â”€ inventory\_events/
â”‚       â””â”€â”€ fact\_sales\_enriched/
â”œâ”€â”€ 5\_scd\_dimension\_modeling/
â”‚   â”œâ”€â”€ dim\_product\_scd1.py
â”‚   â”œâ”€â”€ dim\_product\_scd2.py
â”‚   â”œâ”€â”€ product\_master.csv
â”‚   â””â”€â”€ dim\_product\_history.csv
â”œâ”€â”€ 6\_analytics/
â”‚   â”œâ”€â”€ query\_revenue\_by\_category.sql
â”‚   â”œâ”€â”€ query\_top\_products.sql
â”‚   â””â”€â”€ query\_by\_region.sql
â”œâ”€â”€ 7\_vertex\_ai\_ml\_pipeline/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ model.joblib
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”œâ”€â”€ task.py
â”‚   â”‚   â””â”€â”€ setup.py
â”‚   â””â”€â”€ .github/
â”‚       â””â”€â”€ workflows/
â”‚           â””â”€â”€ train\_and\_deploy.yml
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ etl\_pipeline\_colab.ipynb
â””â”€â”€ README.md

```

---

## ğŸš€ Technologies Used

| Tool / Framework      | Role                                    |
|-----------------------|------------------------------------------|
| PySpark               | Distributed ETL and SQL queries          |
| MongoDB Atlas         | NoSQL ingestion of simulated logs        |
| Google Colab          | Interactive development + testing        |
| Google Drive          | Storage for partitioned Parquet files    |
| Google Cloud Storage  | Model artifact storage                   |
| Vertex AI             | Model deployment and endpoint serving    |
| Feast                 | Feature Store integration (ML pipelines) |
| GitHub Actions        | CI/CD automation                         |
| Cloud Build           | Trigger pipeline builds from GitHub      |

---

## ğŸ§  Skills Demonstrated

- ğŸ”§ Data engineering (ETL, partitioning, PySpark)
- ğŸ—ï¸ Warehouse modeling (star schema, SCD1 & SCD2)
- ğŸ§® Spark SQL for OLAP-style queries
- â˜ï¸ End-to-end MLOps on Google Cloud
- ğŸ“¦ CI/CD with GitHub Actions and Cloud Build
- ğŸ“Š Business insight generation from transactional data

---

## ğŸ‘¨â€ğŸ’» Author

**Ajay Sreekumar**  
AI Research Engineer | Data Science | MLOps  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/ajay-sreekumar-nmims/) | ğŸ§  [Projects Portfolio](https://ajaysreekumar47.github.io/)  
