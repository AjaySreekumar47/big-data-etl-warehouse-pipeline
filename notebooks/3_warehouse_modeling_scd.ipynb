{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üß± 3. Warehouse Modeling with SCD\n",
        " In this notebook, we:\n",
        " - Build dimension and fact tables in a star schema.\n",
        " - Apply SCD Type 1 and Type 2 logic to the `dim_product` table.\n",
        " - Save the modeled tables as optimized Parquet files.\n",
        "\n",
        "### ‚öôÔ∏è Setup & Spark Session"
      ],
      "metadata": {
        "id": "2Oc0b4K7sR2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2 --quiet\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Warehouse Modeling\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "warehouse_path = \"/content/big_data_etl_project/4_data_warehouse/warehouse\"\n",
        "output_path = \"/content/big_data_etl_project/5_scd_dimension_modeling\""
      ],
      "metadata": {
        "id": "UboJd2v3sygA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ Step 1: Load Cleaned Parquet Data"
      ],
      "metadata": {
        "id": "W_w4aPz7s1uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales = spark.read.parquet(os.path.join(warehouse_path, \"sales_logs\"))\n",
        "df_activity = spark.read.parquet(os.path.join(warehouse_path, \"user_activity_logs\"))\n",
        "df_inventory = spark.read.parquet(os.path.join(warehouse_path, \"inventory_events\"))\n",
        "\n",
        "df_sales.printSchema()\n",
        "df_sales.show(2)"
      ],
      "metadata": {
        "id": "5ye2QUCBs4IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß± Step 2: Create Basic Dimensions"
      ],
      "metadata": {
        "id": "UPqF4TfRs6Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, monotonically_increasing_id\n",
        "\n",
        "# -- dim_region --\n",
        "dim_region = df_sales.select(\"region\").distinct().withColumn(\"region_id\", monotonically_increasing_id())\n",
        "\n",
        "# -- dim_time --\n",
        "from pyspark.sql.functions import year, month, dayofmonth\n",
        "\n",
        "dim_time = df_sales.select(\"timestamp\").distinct() \\\n",
        "    .withColumn(\"year\", year(\"timestamp\")) \\\n",
        "    .withColumn(\"month\", month(\"timestamp\")) \\\n",
        "    .withColumn(\"day\", dayofmonth(\"timestamp\")) \\\n",
        "    .withColumn(\"time_id\", monotonically_increasing_id())\n",
        "\n",
        "# -- dim_customer --\n",
        "dim_customer = df_sales.select(\"customer_id\").distinct().withColumn(\"customer_key\", monotonically_increasing_id())\n",
        "\n",
        "# Save basic dimensions\n",
        "dim_region.write.mode(\"overwrite\").parquet(os.path.join(output_path, \"dim_region\"))\n",
        "dim_time.write.mode(\"overwrite\").parquet(os.path.join(output_path, \"dim_time\"))\n",
        "dim_customer.write.mode(\"overwrite\").parquet(os.path.join(output_path, \"dim_customer\"))\n",
        "\n",
        "print(\"‚úÖ Saved dim_region, dim_time, dim_customer\")"
      ],
      "metadata": {
        "id": "bUZi1htes8pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ Step 3: SCD Type 1 ‚Äì Overwrite Product Master"
      ],
      "metadata": {
        "id": "s_58nlK3s-qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
        "\n",
        "product_schema = StructType([\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"product_name\", StringType(), True),\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"price\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "product_master_data = [\n",
        "    (\"p1\", \"iPhone 13\", \"Electronics\", 999.99),\n",
        "    (\"p2\", \"Airpods Pro\", \"Electronics\", 249.99),\n",
        "    (\"p3\", \"Office Chair\", \"Furniture\", 189.99)\n",
        "]\n",
        "\n",
        "df_product_master = spark.createDataFrame(product_master_data, schema=product_schema)\n",
        "df_product_master.write.mode(\"overwrite\").parquet(os.path.join(output_path, \"dim_product_scd1\"))\n",
        "\n",
        "print(\"‚úÖ Saved dim_product_scd1 (SCD Type 1)\")"
      ],
      "metadata": {
        "id": "KwL1Yx1ptBDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üïì Step 4: SCD Type 2 ‚Äì Product with History"
      ],
      "metadata": {
        "id": "y6c5tiPttFVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from datetime import datetime\n",
        "\n",
        "history_data = [\n",
        "    Row(product_id=\"p1\", product_name=\"iPhone 12\", category=\"Electronics\", price=899.99,\n",
        "        valid_from=\"2021-01-01\", valid_to=\"2022-09-13\", is_current=False),\n",
        "    Row(product_id=\"p1\", product_name=\"iPhone 13\", category=\"Electronics\", price=999.99,\n",
        "        valid_from=\"2022-09-14\", valid_to=None, is_current=True),\n",
        "    Row(product_id=\"p3\", product_name=\"Office Chair\", category=\"Furniture\", price=179.99,\n",
        "        valid_from=\"2021-01-01\", valid_to=\"2023-01-01\", is_current=False),\n",
        "    Row(product_id=\"p3\", product_name=\"Office Chair\", category=\"Furniture\", price=189.99,\n",
        "        valid_from=\"2023-01-02\", valid_to=None, is_current=True)\n",
        "]\n",
        "\n",
        "df_product_scd2 = spark.createDataFrame(history_data)\n",
        "df_product_scd2.write.mode(\"overwrite\").parquet(os.path.join(output_path, \"dim_product_scd2\"))\n",
        "\n",
        "print(\"‚úÖ Saved dim_product_scd2 (SCD Type 2)\")"
      ],
      "metadata": {
        "id": "qdP0xAhAtIJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßÆ Step 5: Create Enriched Fact Table"
      ],
      "metadata": {
        "id": "wlRMzCevtKgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "fact_sales_enriched = df_sales \\\n",
        "    .join(dim_region, on=\"region\", how=\"left\") \\\n",
        "    .join(dim_customer, on=\"customer_id\", how=\"left\") \\\n",
        "    .join(dim_time, on=\"timestamp\", how=\"left\") \\\n",
        "    .select(\n",
        "        \"timestamp\", \"customer_id\", \"product_id\", \"region\", \"price\", \"quantity\", \"payment_type\",\n",
        "        \"region_id\", \"customer_key\", \"time_id\"\n",
        "    )\n",
        "\n",
        "fact_sales_enriched.write.mode(\"overwrite\").parquet(os.path.join(output_path, \"fact_sales_enriched\"))\n",
        "print(\"‚úÖ Saved enriched fact_sales table\")"
      ],
      "metadata": {
        "id": "ROC0RKfLtMv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Summary\n",
        " - Built dimensions: `region`, `time`, `customer`, and `product` (SCD1 & SCD2)\n",
        " - Created enriched `fact_sales` table with surrogate keys\n",
        " - Warehouse is now ready for analytics and ML pipeline development\n"
      ],
      "metadata": {
        "id": "DBxRsvN4tObE"
      }
    }
  ]
}