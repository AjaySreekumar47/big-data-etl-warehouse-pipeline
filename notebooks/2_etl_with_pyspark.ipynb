{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1604277",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è 2. ETL with PySpark\n",
    "This notebook extracts logs from MongoDB and applies cleaning + transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598b842",
   "metadata": {},
   "source": [
    "## üîß Install Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b53cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install openjdk-11-jdk-headless -qq\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "!tar -xzf spark-3.3.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8e799",
   "metadata": {},
   "source": [
    "## üîå Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08537509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder     .appName(\"MongoDB_ETL\")     .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\")     .config(\"spark.mongodb.read.connection.uri\", \"<mongo-uri>\")     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cbfc2",
   "metadata": {},
   "source": [
    "## üßº Clean sales_logs and write as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.format(\"mongodb\").option(\"database\", \"enterprise_logs\").option(\"collection\", \"sales_logs\").load()\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "sales_clean = sales_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))     .withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))     .withColumn(\"price\", col(\"price\").cast(\"double\")).drop(\"_id\")\n",
    "sales_clean.write.partitionBy(\"region\").parquet(\"warehouse/sales_logs\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
