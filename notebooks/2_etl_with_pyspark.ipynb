{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 🔁 2. ETL with PySpark from MongoDB\n",
        "This notebook loads data from MongoDB Atlas, performs cleaning and transformations using **PySpark**, and saves the output as partitioned **Parquet** files for downstream data warehouse modeling."
      ],
      "metadata": {
        "id": "iP2fu3QKrJZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚙️ Setup PySpark + Mongo Connector"
      ],
      "metadata": {
        "id": "rGYsPCwXrdPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.1.2 pymongo dnspython --quiet\n",
        "\n",
        "# Install MongoDB Spark Connector JAR\n",
        "import os\n",
        "\n",
        "spark_jars_dir = \"/content/jars\"\n",
        "os.makedirs(spark_jars_dir, exist_ok=True)\n",
        "mongo_jar_url = \"https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar\"\n",
        "mongo_jar_path = os.path.join(spark_jars_dir, \"mongo-spark-connector_2.12-3.0.1.jar\")\n",
        "\n",
        "if not os.path.exists(mongo_jar_path):\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(mongo_jar_url, mongo_jar_path)\n",
        "\n",
        "print(\"✅ Spark + Mongo connector ready.\")"
      ],
      "metadata": {
        "id": "7Gk81_slrhL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 Step 1: Initialize Spark Session"
      ],
      "metadata": {
        "id": "akmVGpPOrkei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETL from MongoDB\") \\\n",
        "    .config(\"spark.jars\", mongo_jar_path) \\\n",
        "    .config(\"spark.mongodb.input.uri\", \"mongodb+srv://<username>:<password>@<cluster>.mongodb.net/enterprise_logs\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"✅ Spark session started.\")"
      ],
      "metadata": {
        "id": "7c0H0m6jrqMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📥 Step 2: Load Collections from MongoDB"
      ],
      "metadata": {
        "id": "tlKPBCzVrsi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_collection(collection_name):\n",
        "    return spark.read.format(\"mongo\").option(\"uri\", f\"mongodb+srv://<username>:<password>@<cluster>.mongodb.net/enterprise_logs.{collection_name}\").load()\n",
        "\n",
        "df_sales = load_collection(\"sales_logs\")\n",
        "df_activity = load_collection(\"user_activity_logs\")\n",
        "df_inventory = load_collection(\"inventory_events\")\n",
        "\n",
        "df_sales.printSchema()\n",
        "df_sales.show(2)"
      ],
      "metadata": {
        "id": "druE7gixrwIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧹 Step 3: Data Cleaning & Transformation"
      ],
      "metadata": {
        "id": "tBSOt2xArzHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# --- Clean Sales Logs ---\n",
        "df_sales_clean = df_sales \\\n",
        "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
        "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
        "    .withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
        "\n",
        "# --- Clean Activity Logs ---\n",
        "df_activity_clean = df_activity \\\n",
        "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
        "    .withColumn(\"session_duration\", col(\"session_duration\").cast(\"double\"))\n",
        "\n",
        "# --- Clean Inventory Logs ---\n",
        "df_inventory_clean = df_inventory \\\n",
        "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
        "    .withColumn(\"quantity_added\", col(\"quantity_added\").cast(\"int\")) \\\n",
        "    .withColumn(\"quantity_removed\", col(\"quantity_removed\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "ntJW-4wrr2Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💾 Step 4: Save as Partitioned Parquet Files"
      ],
      "metadata": {
        "id": "agZzXAGSr5dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_base = \"/content/big_data_etl_project/4_data_warehouse/warehouse\"\n",
        "\n",
        "# --- Sales Logs → partition by region ---\n",
        "df_sales_clean.write.partitionBy(\"region\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(os.path.join(output_base, \"sales_logs\"))\n",
        "\n",
        "# --- Activity Logs → partition by device ---\n",
        "df_activity_clean.write.partitionBy(\"device\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(os.path.join(output_base, \"user_activity_logs\"))\n",
        "\n",
        "# --- Inventory Logs → partition by warehouse_id ---\n",
        "df_inventory_clean.write.partitionBy(\"warehouse_id\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(os.path.join(output_base, \"inventory_events\"))\n",
        "\n",
        "print(\"✅ Partitioned Parquet files saved.\")"
      ],
      "metadata": {
        "id": "1l7cGJlor8RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Summary\n",
        "- MongoDB collections successfully loaded using PySpark.\n",
        "- Cleaned and typecast all fields.\n",
        "- Saved partitioned Parquet files for each log type:\n",
        "   - `sales_logs` → by `region`\n",
        "   - `user_activity_logs` → by `device`\n",
        "   - `inventory_events` → by `warehouse_id`\n",
        "- Data is now ready for Data Warehouse modeling.\n"
      ],
      "metadata": {
        "id": "lqUTitI5r_Qu"
      }
    }
  ]
}